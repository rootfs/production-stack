# Example 1: Gateway Configuration
apiVersion: production-stack.vllm.ai/v1alpha1
kind: InferenceGateway
metadata:
  name: llm-gateway
spec:
  serviceDiscovery: "static"
  routeStrategy: "round-robin"
  routes:
    - modelName: "llama3-70b"
      serviceRef:
        name: "llama3-inference"
        apiGroup: "production-stack.vllm.ai"
        kind: "InferenceService"
---
# Example 2: Inference Service with Speculative Decoding
apiVersion: production-stack.vllm.ai/v1alpha1
kind: InferenceService
metadata:
  name: llama3-inference
  namespace: default
spec:
  modelName: "meta-llama/Llama-2-70b-chat-hf"
  engine: "vllm"
  engineConfig:
    tensor-parallel-size: "4"
    max-num-batched-tokens: "8192"
    max-num-seqs: "256"
  backendRef:
    name: "llama3-backend"
    namespace: "default"
    port: 8000
  speculativeDecodingRef:
    name: "llama3-sd"
    apiGroup: "production-stack.vllm.ai"
    kind: "SpeculativeDecoding"
---
# Example 3: Speculative Decoding Configuration
apiVersion: production-stack.vllm.ai/v1alpha1
kind: SpeculativeDecoding
metadata:
  name: llama3-sd
  namespace: default
spec:
  speculationType: "draft"  # Options: draft, ngram, mlp, eagle
  draftModel: "meta-llama/Llama-2-7b-chat-hf"
  numSpeculativeTokens: 5
  draftTensorParallelSize: 1
  draftModelConfig:
    temperature: "0.8"
    max_tokens: "10"
---
# Example 4: Backend Service
apiVersion: v1
kind: Service
metadata:
  name: llama3-backend
  namespace: default
spec:
  ports:
    - name: http
      port: 8000
      targetPort: 8000
  selector:
    app: llama3-inference
